#  **Below is the Escalation Matrix to be followed for Data Monitoring alert**
    
**Day 1: Initial Notification**
*   **Action:** Drop a note to stakeholders informing them of the issue. Drop a follow up note after 4 hours in case of no response. This will showcase the urgency.
*   **If no response, follow Day 2:**

       1.  No response received over emails.
       2.  Need further clarification.

**Day 2: Further Troubleshooting | First Follow-Up | working session -1 (include Shweta & Yash)**
  1. Bring the stakeholders to a Teams chat.
   2.  If the issue is not resolved in the Teams chat, schedule time/call them immediately if available.
  3. Bring the issue to the team's attention.

**Day 3: Call with the Stakeholders | working session - 2**
   1.  In case the issue persists, loop in other teams such as firewall, firewall engineering/ network team if the issue is connectivity.
   2.  Keep the email thread updated with the progress.
   3.  If the issue persists, escalate it to Yash/Shweta.

**Day 4: Escalate to Higher Management**
     
   *   Bring the issue to Lauri Ann/Danny's/BISO attention to get things moving and help resolve the issue.

**Day 5: Working Session**
*   If the issue is not resolved, set up another call with stakeholders as a working session.

**Day 6: Final Steps**
*   Continue monitoring and troubleshooting until the issue is resolved. Document all steps taken and communicate with stakeholders regularly.
By following this escalation matrix, you can ensure a structured and timely response to the index feed issues in Splunk, aiming to resolve them efficiently within the 7-day window.